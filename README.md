# HPC Lab 2 ‚Äì Hybrid MPI + OpenMP Matrix Multiplication  
Parallel Matrix Multiplication on Titan Supercomputer & Local Machine

## üìå Overview
This project implements and benchmarks three hybrid **MPI + OpenMP** matrix-multiplication programs written in C.  
The goal was to evaluate runtime scalability across:

- **Local machine:** Apple Silicon M3 MacBook Pro  
- **Titan Supercomputer:** SB compute nodes (multi-CPU, multicore)  
- Matrix sizes up to **6000 √ó 6000**

The analysis compares how performance changes as we scale **nodes**, **cores per node**, and **OpenMP parallelization strategy**.

---

## ‚öôÔ∏è Implementations Included

### **1. OMP (Baseline Hybrid Version)**
- MPI handles data distribution (scatter + broadcast + gather)
- OpenMP parallelizes the innermost dot-product loop  
- File: `mmmpiOMP.c`

### **2. OMP2 (Improved Inner-Loop Parallelization)**
- More efficient OpenMP region placement  
- Reduced thread scheduling overhead  
- File: `mmmpiOMP2.c`

### **3. OMP3 (Outer-Loop Parallelization ‚Äî Fastest)**
- OpenMP parallelizes at the row level  
- Maximizes concurrency for large matrices  
- Best performance across all Titan runs  
- File: `mmmpiOMP3.c`

---

## üß† Key Concepts Demonstrated
- Hybrid **MPI + OpenMP** programming  
- Distributed-memory row partitioning  
- Shared-memory multithreading inside compute kernels  
- Cache and NUMA behavior in HPC nodes  
- Strong scaling across nodes and cores  
- SLURM job scheduling and cluster execution  

---

## üß™ Experimental Setup

### **Systems Used**

#### **MacBook Pro (M3, 2023)**
- High per-core performance  
- Unified memory  
- Excellent for small matrix benchmarks  

#### **Titan SB Node**
- Dual Intel Xeon CPUs  
- 40+ cores per node  
- DDR4 memory subsystem  
- Ideal for large distributed workloads  

### **Matrix Sizes Tested**
- 1000 √ó 1000  
- 2000 √ó 2000  
- 4000 √ó 4000  
- 6000 √ó 6000  

### **Parallel Configurations (Titan)**
Nodes √ó Cores per Node:
- 1√ó1, 1√ó2, 1√ó4, 1√ó8, 1√ó16  
- 8√ó8, 8√ó16  
- 16√ó8, 16√ó16  

---

## üöÄ Fastest Configurations (OMP3)
Used with:

- **8 nodes √ó 16 cores**
- **16 nodes √ó 16 cores**

---

## üíª Laptop vs üñ•Ô∏è Titan
**The M3 laptop outperformed Titan on smaller matrices (‚âà1000√ó1000) due to:**

- Lower MPI overhead  
- Faster unified memory  
- Higher per-core performance  
- Titan‚Äôs communication startup cost dominating small workloads  

---

## üßµ OMP2 vs OMP3
- **OMP2** ‚Üí Minimal improvement beyond ~8 cores  
- **OMP3** ‚Üí Truly scalable; best performance for large matrices  

---

## ‚ñ∂Ô∏è Compilation & Execution (Titan)

### **Compile**
```bash
mpiicx -O2 -qopenmp mmmpiOMP.c  -o mmmpiOMP
mpiicx -O2 -qopenmp mmmpiOMP2.c -o mmmpiOMP2
mpiicx -O2 -qopenmp mmmpiOMP3.c -o mmmpiOMP3
```

## ‚ñ∂Ô∏è Run with SLURM

**Example ‚Äî 8 nodes √ó 16 cores per node:**

```bash
sbatch --partition=sb -n 8 --ntasks-per-node=1 -c 16 \
       mmmpi.bat mmmpiOMP3 6000 1
```

**Program Arguments**
  - Executable name
  - Matrix size
  - Transpose flag (1 = enabled)

## üìù Deliverables Included
- Parallel C source code
- Full performance analysis report
- Charts & benchmark tables
- Final PowerPoint summary presentation

## üéØ What This Project Demonstrates
- Ability to write optimized parallel C
- Deep understanding of MPI communication
- Efficient use of OpenMP multithreading
- Real HPC experience on a multi-node cluster
- Strong-scaling and performance interpretation
- Memory hierarchy & cache-aware reasoning
- Clean benchmarking and reproducible experiments
